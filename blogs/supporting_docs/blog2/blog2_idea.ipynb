{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Webscraping Blog"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is Web Scraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Web Scraping is the process of extracting data froma website. Collecting product prices off of Amazon or movie ratings off of IMDB would both be forms of web scraping. On a typical user-facing website, data is embedded in a web page in a way that is easy to interpret but hard to extract and consolidate by hand. \n",
    "\n",
    "Web scrapers are tools that can automate the process of data collection from a browser. They serve as an interface between the web browser and the scraping instructions. The instructions can be written using most programming languages with some langauges having dedicated libraries for webscraping. Below are two examples of web scraping tools."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selenium"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selenium WebDriver\n",
    "Selenium is a webdriver designed to automate web browsing. It simulates normal user interaction with a web browser but interfaces with coding languages. This allows specific and complex instructions to be fed in and run.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting Up\n",
    "Using Selenium with Python requires downloading the required library using pip\n",
    "\n",
    "```\n",
    "pip install selenium\n",
    "```\n",
    "\n",
    "In addition, the browser driver also needs to be installed which will depend on the browser being utilized (Chrome, Firefox, Internet Explorer). That is all the setup required to experiment with the web driver. In the case of Google Chrome, Google supports and updates the Chrome driver. The chrome web driver can be found here:\n",
    "\n",
    "<https://chromedriver.chromium.org/downloads>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experimenting with Selenium"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this experiment is to open youtube in a chrome browser and search for casey neistat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Open chrome webdriver\n",
    "with webdriver.Chrome('/Users/USERNAME/Desktop/Programming/seleniumdrivers/chromedriver') as browser:\n",
    "\n",
    "    #tells driver to wait 3 seconds\n",
    "    wait = WebDriverWait(browser,3)\n",
    "    #checks for visibility of element on webpage\n",
    "    visible = EC.visibility_of_element_located\n",
    "\n",
    "    # opens chrome browser to youtube\n",
    "    browser.get('https://youtube.com')\n",
    "\n",
    "    #waits 3 seconds before continuing code\n",
    "    time.sleep(3)\n",
    "\n",
    "    # wait 3 seconds or until search_query is visible in html\n",
    "    wait.until(visible((By.NAME, \"search_query\")))\n",
    "\n",
    "    # clicks on search query\n",
    "    browser.find_element_by_name('search_query').click()\n",
    "\n",
    "    # types in casey neistat into search query\n",
    "    browser.find_element_by_name('search_query').send_keys('casey neistat')\n",
    "\n",
    "    # clicks search button\n",
    "    browser.find_element_by_id('search-icon-legacy').click()\n",
    "\n",
    "    time.sleep(3)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above code will open up Youtube in a Chrome browser and search for casey neistat. This is utilizing both the installed selenium library and the chrome driver executable. Selenium will show the scraping process in real time instead of just returning an output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beautiful Soup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beautiful Soup is a webscraping Python library. Instead of realtime, visible navigation the browser is navigated behind the scenes using the request library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experimenting with Beautiful Soup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this exercize is to retrieve data from a mock bookstore website and store the book data in a dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports and Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def retrieve_titles(soup_):\n",
    "    '''\n",
    "    retrieves the titles of books from the webpage\n",
    "    '''\n",
    "    marker = soup_.find('ol',class_='row')\n",
    "    list_of_titles = []\n",
    "    for book in marker.findAll('h3'):\n",
    "        list_of_titles.append(book.find('a').attrs['title'])\n",
    "        \n",
    "    return list_of_titles\n",
    "\n",
    "def retrieve_ratings(soup_):\n",
    "    '''\n",
    "    retrieves the ratings of books from the webpage\n",
    "    '''\n",
    "    marker = soup_.find('ol',class_='row')\n",
    "    book_ratings = []\n",
    "    for p in marker.findAll('p'):\n",
    "        if p.attrs['class'][-1] == 'One':\n",
    "            book_ratings.append(1)\n",
    "        elif p.attrs['class'][-1] == 'Two':\n",
    "            book_ratings.append(2)\n",
    "        elif p.attrs['class'][-1] == 'Three':\n",
    "            book_ratings.append(3)\n",
    "        elif p.attrs['class'][-1] == 'Four':\n",
    "            book_ratings.append(4)\n",
    "        elif p.attrs['class'][-1] == 'Five':\n",
    "            book_ratings.append(5)\n",
    "    return book_ratings\n",
    "\n",
    "def retrieve_prices(soup_):\n",
    "    '''\n",
    "    retrieves the prices of books from webpage\n",
    "    '''\n",
    "    marker = soup_.find('ol', class_='row').findAll('p',class_='price_color')\n",
    "    price_list = []\n",
    "    for price in marker:\n",
    "        price_list.append(float(price.text[1:]))\n",
    "    \n",
    "    return price_list\n",
    "\n",
    "def retrieve_availabilities(soup_):\n",
    "    '''\n",
    "    retrieves the availability of a book from the webpage\n",
    "    '''\n",
    "\n",
    "    book_statuses = soup_.findAll('i',class_='icon-ok')\n",
    "    book_status_list = []\n",
    "    \n",
    "    for book_status in book_statuses:\n",
    "        book_status_list.append(book_status.nextSibling.replace('\\n','').strip())\n",
    "\n",
    "    return book_status_list\n",
    "\n",
    "def go_to_next_page(soup_):\n",
    "    '''\n",
    "    moves to next page of the webpage\n",
    "    '''\n",
    "    button = soup_.find('li', class_='next').find('a').attrs['href']\n",
    "    is_next = soup_.find('li', class_ = 'next').text == 'next'\n",
    "    \n",
    "    if button[0] == 'c':\n",
    "        return [button,is_next]\n",
    "    else:\n",
    "        return ['catalogue/' + button , is_next]\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieving Data from Website"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Go to mock webpage and retrieve book data. Store data as a pandas dataframe\n",
    "base_html = 'http://books.toscrape.com/'\n",
    "next_page_html = ''\n",
    "df = pd.DataFrame()\n",
    "html_page = requests.get(base_html + next_page_html)\n",
    "soup = BeautifulSoup(html_page.content,'html.parser')\n",
    "\n",
    "# build initial dataframe with website homepage\n",
    "df = pd.DataFrame({'title':retrieve_titles(soup),\n",
    "                   'rating': retrieve_ratings(soup),\n",
    "                   'price': retrieve_prices(soup),\n",
    "                   'availability': retrieve_availabilities(soup)})\n",
    "\n",
    "# loop through each consecutive page of books  and add to dataframe\n",
    "for i in range(49):\n",
    "    try:\n",
    "        if go_to_next_page(soup)[1]:\n",
    "            next_page_html = go_to_next_page(soup)[0]\n",
    "            html_page = requests.get(base_html+next_page_html)\n",
    "            soup = BeautifulSoup(html_page.content,'html.parser')\n",
    "            df = pd.concat([df, pd.DataFrame({'title':retrieve_titles(soup),\n",
    "                                              'rating': retrieve_ratings(soup),\n",
    "                                              'price': retrieve_prices(soup),\n",
    "                                              'availability': retrieve_availabilities(soup)})],axis=0)\n",
    "    except:\n",
    "        # print page where error was returned\n",
    "        print(base_html+next_page_html)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1000, 4)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>price</th>\n",
       "      <th>availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>3</td>\n",
       "      <td>51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>1</td>\n",
       "      <td>53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>1</td>\n",
       "      <td>50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>4</td>\n",
       "      <td>47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>5</td>\n",
       "      <td>54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  rating  price availability\n",
       "0                   A Light in the Attic       3  51.77     In stock\n",
       "1                     Tipping the Velvet       1  53.74     In stock\n",
       "2                             Soumission       1  50.10     In stock\n",
       "3                          Sharp Objects       4  47.82     In stock\n",
       "4  Sapiens: A Brief History of Humankind       5  54.23     In stock"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best Practices"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before continuing with some addtional instructions let's first review the ethics and best practices surrounding web scraping.\n",
    "\n",
    "When considering web scraping it's important to first identify the purpose of the data being collected and what data needs to be collected. Once these items are determined it is easier to figure out if scraping is necessary at all. Some research should be done to see if the data to be scraped can be found on a public API. If scraping is necessary, sensitive and proprietary data should not be included. It would be better to reach out to a site owner directly for access if sensitive data is necessary. Finally, care should be taken to not hurt the performance of a website by disrupting normal website traffic with too many server requests.\n",
    "\n",
    "Web scraping is a powerful tool that if respected can save a data scientist a lot of time with data collection."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}